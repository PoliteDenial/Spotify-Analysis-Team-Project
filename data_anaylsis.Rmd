---
title: "Spotify Data Anaylsis"
date: "3/16/2022"
output: "html_document"
html_document: default
pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, warning=FALSE}
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lubridate)
library(randomForest)
library(igraph)
library(spotifyr) #spotify wrapper class
library(glmnet)#A bunch of kinds of regression models (Use elastic net)
library(caret)#optimizing random forest and a bunch of other ml tools
library(reshape2)#cleaning a data frame
library(ModelMetrics)#for use when computing and comparing by hand model metrics
library(ggdark)#dark ggplot theme
```



Spotify API access token wrapper class for get rank function
```{r}
id ='71e950603dd64e1291d04fabafee04d9'
secret = '4a8f958cd10448e7aba2aaed03280140'
Sys.setenv(SPOTIFY_CLIENT_ID = id)
Sys.setenv(SPOTIFY_CLIENT_SECRET = secret)
access_token <- get_spotify_access_token() 
```


```{r}
set.seed(2022) #Controlled random
dpath = "clean_data/trackData.csv"#CSV path
data = read.csv(dpath)#Import data from CSV in the same directory

#////////////////////////////////////////////////////////////////////////////////////////////
#Are we using pure ranking or ranking avg rank week? 
#1 if avg rank 0 if just rank
rank_xor = 0
#WARNING: If predicing average ranking then every refrence to ranking should be changed to rankingX!
#If you don't do that none of the code will execute this if statment simply toggles the processing of the data frame.
if(rank_xor==1){
    avg = data %>% 
    group_by(song_name)  %>%  
    summarise(s_freq = sum(ranking), weeks_trending = n()) %>% 
    mutate(ranking = s_freq / weeks_trending)
    data = merge(avg, data, by="song_name")
    data = select(data, -c(week, ranking, id,X,s_freq))
    data=distinct(data)

}



```





Tree graphing function from [here](https://www.r-bloggers.com/2017/03/plotting-trees-from-random-forest-models-with-ggraph/) has been modified to fit our needs. That being said we did not write any of code and all credit goes to the original author. 
```{r warn = -1}
library(ggraph)
library(igraph)
tree_func <- function(final_model, 
                      tree_num) {
  
  # get tree by index
  tree <- randomForest::getTree(final_model, 
                                k = tree_num, 
                                labelVar = TRUE) %>%
    tibble::rownames_to_column() %>%
    # make leaf split points to NA, so the 0s won't get plotted
    mutate(`split point` = ifelse(is.na(prediction), `split point`, NA))
  
  # prepare data frame for graph
  graph_frame <- data.frame(from = rep(tree$rowname, 2),
                            to = c(tree$`left daughter`, tree$`right daughter`))
  
  # convert to graph and delete the last node that we don't want to plot
  graph <- graph_from_data_frame(graph_frame) %>%
    delete_vertices("0")
  
  # set node labels
  V(graph)$node_label <- gsub("_", " ", as.character(tree$`split var`))
  V(graph)$leaf_label <- as.character(tree$prediction)
  V(graph)$split <- as.character(round(tree$`split point`, digits = 2))
  
  # plot
  plot <- ggraph(graph, 'dendrogram') + 
    geom_edge_link() +
    geom_node_point() +
    geom_node_text(aes(label = node_label), na.rm = TRUE, repel = TRUE) +
    geom_node_label(aes(label = split), vjust = 2.5, na.rm = TRUE, fill = "white") +
    theme(panel.grid.minor = element_blank(),
          panel.grid.major = element_blank(),
          panel.background = element_blank(),
          plot.background = element_rect(fill = "white"),
          panel.border = element_blank(),
          axis.line = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank(),
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          plot.title = element_text(size = 18)) + dark_theme_dark()
  
  print(plot)
}
```

This cell contains primarily contains functions that we use throughout the anaylsis process:
```{r warn = -1}
#////////////////////////////////////////////////////////////////////////////////////////////////

#Searchs spotify for a song and given a model predict is value using that model
#model_type refrence:
# elastic net/ lasso/ridge/linear/log/etc = 1
# random forest = 2
# If it requires a lamda value then chose 1 otherwise chose 2

getEstimatedRank = function(song_query = "", mf = "", modelObj,  model_type = 0, lambda = 0)
{
  new = spotifyr::search_spotify(song_query,type = "track")
  id = new["id"][[1]][1]
  track = get_track_audio_features(id)
  track = track[mf]
  new_track = matrix(as_vector(track),nrow = 1, ncol = length(mf))
  
  if(model_type == 1){
    p_vec = predict(modelObj,s = lambda, newx = new_track)
      return(cat("\n////////////////////////////////////////////",
      "\nGiven song ",
      song_query, 
      "\nThe model predicted the value: ",
      p_vec,
      "\n////////////////////////////////////////////\n"))
  }else if(model_type == 2){
    p_vec = as.vector(predict(modelObj, newx = new_track))
    t_mean= mean(tail(p_vec, n=50))
      return(cat("\n////////////////////////////////////////////",
      "\nGiven song ",
      song_query, 
      "\nThe model predicted the value: ",
      t_mean,
      "\n////////////////////////////////////////////\n"))
  }else{
    cat("\nInvalid model type!","\nTry 1 or 2")
  }
  
}
#////////////////////////////////////////////////////////////////////////////////////////////////
#convert a set of values to 0,1  given a cutoff value ie 10 <= 50 therefore return 1
toBinary = function(data, cut_off) 
{
  binaryRep = sapply(data, function(rank){
    ifelse(rank < cut_off, 1, 0)
  })
  return(binaryRep)
}
#////////////////////////////////////////////////////////////////////////////////////////////////
#Give the lower triangle of a corrlation matrix
getLow = function(corMatrix) 
{
  corMatrix[upper.tri(corMatrix)] = NA
  return(corMatrix)
}
#Calculate and print mse, rmse, normalized rmse, and r squared value to console 
get_metrics = function(Actual, Predicted, model, mf, rf = 0){
  
  vA= sum((Actual-mean(Actual))^2)/length(Actual) 
  vP = var(Predicted)
  #mse = mse(Actual, Predicted)
  mse = mean((Actual - Predicted)^2)
  #rmse = rmse(Actual,Predicted)
  rmse= sqrt(mean((Actual - Predicted)^2))
  norm_rmse = rmse(Actual,Predicted) / (max(Actual)-min(Actual))
  tss = sum((Actual - mean(Actual))^2)
  rss = sum((Predicted-Actual)^2) #sum((Predicted - mean(Predicted))^2)
  if(rf==0)
  {
    r_sq = 1-(rss/tss)
  }
  else 
  {
    r_sq =sqrt(mse/var(Actual))
  }
  top = (1-r_sq)*(length(Actual)-1)
  bottom = (length(Actual)-length(mf)-1)
  adj_r_sq = 1-(top/bottom)
  # sst = sum((Actual - mean(Actual))^2)
  # sse = sum((Predicted - Actual )^2)
  # rsq = 1 - sse/sst
 
  cat("\nGiven Model:",deparse(substitute(model)),"\n",
  "\n////////////////////////////////////////////",
  "\nVariance Actual: ", vA,
  "\nVariance Predicted: ", vP,
  "\nMean squared error: ", mse,
  "\nRoot mean sqaured error: ",rmse,
  "\nNormailized root mean sqaured error: ",norm_rmse,
  "\nR^2/Psuedo-R^2 value of model: ",r_sq)
  #"\nAdjusted R^2 value of model: ",adj_r_sq)
  cat(
  "\n////////////////////////////////////////////")
}


```

A corrlation heatmap of audio features, and rank in our dataframe help determine what features actually have a strong correlation with a "high" rank.  

```{r}
#////////////////////////////////////////////////////////////////////////////////////////////////

#select columns we care about
heat_df = select_if(data, is.numeric)
#generate correlation matrix
cor_mat = round(cor(heat_df),2)
#get lower triangle of a correlation matrix
ltri = getLow(cor_mat)
m_cor_mat_f = melt(ltri, na.rm = 1)

#////////////////////////////////////////////////////////////////////////////////////////////////
#plotting the correlation matrix
corMap = ggplot(m_cor_mat_f, aes(Var1, Var2, fill = value)) + 
  dark_theme_gray() +
  geom_tile(color = "white") +
  scale_fill_gradient2(high = "#f51bc6", low = "#0400ff", mid = "#e7e7e7", 
  midpoint = 0, limit = c(-1,1), space = "Lab", name = "Corralation\nStrength") +
  coord_fixed() + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1))+
  labs(
  x = "Audio Features", 
  y=  "Audio Features", 
  title = "Spotify Audio Features And There Corralations In Denver Colorado"
)

corMap
```

From here we want to take a look at the features that when compared to rank column have the strongest  correlations both negative and postive depending on the way in which the value is formatted. So if we look at the far right column we can see that very clearly there are postive correlations primarily between rank and danceability along with rank and valence. We generally want to look for negative correlations since the lower a songs rank is the closer it is to the number one spot on the charts. That being said within our data set it looks to be the case that energy, loudness, instrumentalness, livelness and duration are correlated with a "high" rank or is more likely to be on the upper section of the charts.


As disscussed above and shown by our correlation heatmap, the most relevant freatures to use to predict a song will be popular are energy, loudness, instrumentalness, livelness, and duration. Additionally to predict that song will be unpopular we also be adding in danceability, and valence to ensure the model is balanced. 

Now that brings us to the task of deciding on what kind machine learning model we should use in order to predict a songs popularity in Denver. Intailly we ended up trying lasso regression to automate the process of selecting features automated however this led to quite poor results. We also tried using ridge regression then elastic net but this also gave us less then perfect results.



The code below is for that ridge/elastic net/lasso regression this and as can be seen in the console the the $R^2$ value is extremly low meaning that this certainly isn't the correct approach for this kind of a data set since it's quite noisey. 

```{r}


#selecting Features 
model_features = c(
 "energy",
 "loudness",
 "instrumentalness",
 "liveness",
 "duration_ms",
 "acousticness",
 "valence",
 "key",
 "time_signature",
 "danceability",
 "tempo"
)

#////////////////////////////////////////////////////////////////////////////////////////////////

#Creating random traning/test partions of our data set with a 70/30 split
train = data

#////////////////////////////////////////////////////////////////////////////////////////////////
train$sample  = sample(c(1,0), replace=1, prob = c(0.7,0.2) , size = nrow(data)) # add sample column
test = train %>% filter( sample %in% c(0)) #partion data
test = test[order(test$ranking),] #clean test for viewing
train = subset(train, sample!=0)  #partition train set
true_rank = test$ranking
#////////////////////////////////////////////////////////////////////////////////////////////////
```



```{r,warn = -1}
#Lasso vars
y <- train$ranking # nolint
x <- data.matrix(train[,model_features])

#////////////////////////////////////////////////////////////////////////////////////////////////

#Define model
cv_model <- cv.glmnet(x,y,alpha = 0.2)
best_lambda <- cv_model$lambda.min

#print coeff
best_lambda
#check model curve
plot(cv_model)

#////////////////////////////////////////////////////////////////////////////////////////////////
elastic_model <- glmnet(x,y,alpha = 0.2, lambda = best_lambda)
coef(elastic_model)

#////////////////////////////////////////////////////////////////////////////////////////////////
test_features = as.matrix(test[,model_features])
#////////////////////////////////////////////////////////////////////////////////////////////////

#make our predctions
y_predicted = predict(elastic_model, s = best_lambda, newx = test_features)
g_cut_off = 50 #cut off point for binary representations

#How well do we perfrom at detecting the top 1:X tracks...
y_hat_binary =toBinary(y_predicted, g_cut_off)
true_rank_binary = toBinary(test$ranking, g_cut_off)
caret::confusionMatrix(as.factor(y_hat_binary),as.factor(true_rank_binary)) 

#////////////////////////////////////////////////////////////////////////////////////////////////

#Compute metrics of elastic_model
get_metrics(true_rank, y_predicted, elastic_model, model_features) 
```


```{r, warn =-1}

#generate list of song index in datat frame
index = c(1:(length(true_rank))) 
#Create dataframe with predictions and index
e_df = data.frame(y_predicted, index)

#////////////////////////////////////////////////////////////////////////////////////////////////
#Plot  regression true_rank vs predicted rank
e_p = ggplot(e_df) + 
geom_point(aes(y=y_predicted, x=index, colour = "Predicted Rank")) +  
geom_line(aes(y=true_rank, x=index, colour = "Actual Rank")) + 
labs(x= "Song Index", y= "ranking", title = "Elastic Model Performance") +
dark_theme_gray()
e_p 
```

The function below allows you to search for a song in the spotify catalog and using one of the models that we created output the prediction of that particular song. This particular call of the function is set to use the elastic net regression model.

```{r}
getEstimatedRank("Good Leo Trix", model_features , elastic_model , 1 , best_lambda)
```

Using random forest for our regression model we get significantly better performance with a RMSE of 22.9 after running 1e5 iterations of the tree.

```{r, warn = -1}
#////////////////////////////////////////////////////////////////////////////////////////////////
#define test set without ranking
test_n_rank =   select(test, -c(ranking))
#define random_forest_model
random_forest_model = randomForest(
  formula = ranking~. ,
  data = train,
  mtree = 10000,
  mtry = 10,
  split = 8,
  type = "regression"
)

#Looking at how well we can predict the top X track ranks 
y_pred_rf = predict(random_forest_model, test_n_rank)
y_pred_rf_binary =toBinary(y_pred_rf, g_cut_off)
caret::confusionMatrix(as.factor(y_pred_rf_binary), as.factor(true_rank_binary))

#Compute metrics of random forest regression
get_metrics(test$ranking,y_pred_rf, random_forest_model,model_features,1)
```

As You can see our random forest model performs quite a bit better then the elastic net reggression.

```{r}

#Plot rf true_rank vs rank
rf_df = data.frame(y_pred_rf, index)

rf_p = ggplot(rf_df) + 
geom_point(aes(y=y_pred_rf, x=index, colour = "Predicted Rank")) +  
geom_line(aes(y=true_rank, x=index, colour = "Actual Rank")) + 
geom_line(aes(y=true_rank, x=index, colour = "Actual Rank")) + 
labs(x= "Song Index", y= "ranking", title = "Random Forest Model Performance") + 
dark_theme_gray()
plot(random_forest_model)
rf_p
#Plot rf tree 1/10
tree_func(random_forest_model, 1)
```

The function below allows you to search for a song in the spotify catalog and using one of the models that we created output the prediction of that particular song. This particular call of the function is set to use the random forest regression model. It is currently broken since predictions of a random forest track ranking is hard extract given such a small input set. 
```{r}
#Song should return a relatively low value but gives the same return regardless of input song
getEstimatedRank("Good Leo Trix", model_features , random_forest_model, 2)
```

With regards to our binary test of top X tracks we should make sure that the distribution of our test isn't skewed. After examining the below graph is it is obvious that the test set of the data was slightly skewed towards worse ranking tracks however, it really is only a slight skewed meaning that our binary representations and therefore our accuracy tests aren't completely off base. However in the future it would be best to sample a normal distribution of values for our test set atleast for this case specifically.
```{r}
hist(test$ranking)
```
